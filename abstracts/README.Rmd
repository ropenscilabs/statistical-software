---
title: "Analyses of Journal of Statistical Software Abstracts"
author: "Mark Padgham"
date: "`r Sys.Date()`"
output: 
    html_document:
        toc: true
        toc_float: true
        number_sections: false
        theme: flatly
---

```{r options, echo = FALSE}
knitr::opts_chunk$set(
  out.width = "100%",
  collapse = TRUE,
  comment = "#>",
  fig.path="figures/"
)
```


## Scrape the Abstracts

This code gets the number of papers for each volume:
```{r num_papers}
library (magrittr)
num_papers <- function (volume = 13) {
    u <- paste0 ("https://www.jstatsoft.org/issue/view/v",
                 sprintf ("%03d", volume))
    x <- xml2::read_html (u)

    # content is sometimes only Articles, but other times Articles, Reviews,
    # Technical Notes, and other categories. Each of this is delineated by
    # a simple div:tocSectionTitle, so these need to be identified, and content
    # parsed only for "Articles".
    content <- rvest::html_nodes (x, "#content") %>%
        rvest::html_children ()
    index <- grep ("tocSectionTitle", rvest::html_attrs (content))
    divs <- rvest::html_text (content [index])
    articles <- grep ("Articles", divs)
    if (articles == 1 & length (index) > 1) {
        content <- content [(index [1] + 1):(index [2] - 1)]
    } else if (articles == length (divs)) {
        content <- content [(index [articles] + 1):length (content)]
    } else {
        content <- content [(index [articles] + 1):(index [articles + 1] - 1)]
    }
    content <- content [grep ("tocArticle", content)]

    #td <- rvest::html_nodes (x, "td") %>%
    #    rvest::html_attrs () %>%
    #    unlist () %>%
    #    unname ()
    #length (which (td == "tocTitle"))
    return (length (content))
}
volume <- 1
npapers <- num_papers (volume)
message ("Volume ", volume, " has ", npapers, " papers")
```

This extracts the title and abstract for one paper:
```{r get_abstract}
get_abstract <- function (volume = 13, paper = 1) {
    u <- paste0 ("https://www.jstatsoft.org/index.php/jss/article/view/v",
                 sprintf ("%03d", volume), "i", sprintf ("%02d", paper))
    x <- xml2::read_html (u)
    # The 2 fields are embedded in meta:
    title <- rvest::html_nodes (x, "meta[name='description']") %>%
        rvest::html_attr ("content")
    abstract <- rvest::html_nodes (x, "meta[name='DC.Description']") %>%
        rvest::html_attr ("content")
    date <- rvest::html_nodes (x, "meta[name='DC.Date.issued']") %>%
        rvest::html_attr ("content")

    # alternative: Extract from body - some volumes/papers don't have the meta:
    if (nchar (abstract) == 0 | nchar (title) == 0) {
        txt <- rvest::html_nodes (x, "td") %>%
            rvest::html_text ()
        title <- txt [grep ("^Title:", txt) + 1]
        abstract <- txt [grep ("^Abstract:", txt) + 1]
    }

    # Volume 76 has final 2 articles with non-standard URLs which fail here
    ret <- NULL
    if (length (abstract) > 0 | length (title) > 0) {
        ret <- list (title = title,
                     abstract = abstract,
                     date = date)
    }
    return (ret)
}
get_abstract (1, 3)
```

Get the current---and therefore maximal---volume number:
```{r current_vol}
current_vol <- function () {
    u <- "https://www.jstatsoft.org/issue/archive"
    x <- xml2::read_html (u)
    n <- rvest::html_nodes (x, "div[id='issues']") %>%
        rvest::html_nodes ("div[id]") %>%
        rvest::html_attrs () %>%
        unlist () %>%
        unname ()
    as.integer (gsub ("issue-", "", n [grep ("issue-", n)]) [1])
}
current_vol ()
```


Get all abstracts:
```{r get_all_abstracts, eval = TRUE}
get_all_abstracts <- function () {
    vmax <- current_vol ()
    res <- tibble::tibble (date = character (),
                           volume = integer (),
                           number = integer (),
                           title = character (),
                           abstract = character ())

    pb <- utils::txtProgressBar (style = 3)
    for (i in seq (vmax)) {
        n <- num_papers (i)
        for (j in seq (n)) {
            a <- get_abstract (i, j)
            if (!is.null (a)) {
                res <- rbind (res, tibble::tibble (date = a$date,
                                                   volume = i,
                                                   number = j,
                                                   title = a$title,
                                                   abstract = a$abstract))
            }
        }
        utils::setTxtProgressBar (pb, i / vmax)
    }
    close (pb)

    return (res)
}
if (!file.exists ("jss-abstracts.Rds")) {
    a <- get_all_abstracts ()
    saveRDS (a, file = "jss-abstracts.Rds")
}
```

## Analyses

These presume all of the above to have been run, and a local
`"jss-abstracts.Rds"` file to have been created.

```{r dfm, eval = TRUE, message = FALSE}
library (magrittr)
library (quanteda)
library (topicmodels)
a <- readRDS ("jss-abstracts.Rds")
q <- corpus (a$abstract)
docvars (q, "year") <- as.integer (substring (a$date, 1, 4))
docvars (q, "volume") <- as.integer (a$volume)
docvars (q, "number") <- as.integer (a$number)

d <- dfm (q, remove = quanteda::stopwords (), remove_punct = TRUE)
```
topic models don't reveal anything:
```{r topic-models, eval = TRUE}
#quanteda::textplot_wordcloud (d)
d <- dfm_trim (d, min_termfreq = 10)
my_lda <- LDA (d, k = 5)
knitr::kable (get_terms (my_lda, 20))
```

spacy-parsed nouns and verbs also don't reveal anything:
```{r spacy, eval = TRUE, message = FALSE}
library (spacyr)
spacy_initialize ()

# extract lists of nouns and verbs:
nv <- pbapply::pblapply (a$abstract, function (i) {
    s <- spacy_parse (i)
    list (noun = s$token [grep ("NOUN", s$pos)],
          verb = s$token [grep ("VERB", s$pos)])
                           })
nouns <- unlist (lapply (nv, function (i) paste0 (i$noun, collapse = " ")))
verbs <- unlist (lapply (nv, function (i) paste0 (i$verb, collapse = " ")))
dn <- dfm (nouns, remove = stopwords (), remove_punct = TRUE)
dv <- dfm (verbs, remove = stopwords (), remove_punct = TRUE)
```

### Word cloud of all nouns

```{r wordcloud-nouns}
textplot_wordcloud (dn, max_words = 200, max_size = 8)
```

### Word cloud of all verbs

```{r wordcloud-verbs}
textplot_wordcloud (dv, max_words = 200, max_size = 9)
```

### Topic Models of Nouns and Verbs

```{r lda}
lda_n <- LDA (dn, k = 5)
knitr::kable (get_terms (lda_n, 10))
lda_v <- LDA (dv, k = 5)
knitr::kable (get_terms (lda_v, 10))
```

... none of those reveal anything useful.

## changes over time


First collate the abstract on annual bases:
```{r annual-collation}
a <- readRDS ("jss-abstracts.Rds")
library (stringr)
# The vectorized version of stringr::str_replace_all is nearly 10 times faster
# than gsub on stops <- paste0 (stops, collapse = "|"). Note that it needs the
# vector of patterns to be named.
collate_abstracts <- function (a) {
    a$year <- as.integer (substring (a$date, 1, 4))
    years <- sort (unique (a$year))
    abstracts <- rep (NA, length (years))
    #stops <- paste0 (paste0 ("\\s", quanteda::stopwords (), "\\s"), collapse = "|")
    stops <- paste0 ("\\s", quanteda::stopwords (), "\\s")
    repl <- rep ("", length (stops))
    names (stops) <- names (repl) <- paste0 (seq_along (stops)) # necessary!
    for (y in seq (years)) {
        ab <- paste0 (a$abstract [a$year == years [y]], collapse = " ")
        #ab <- gsub (stops, " ", ab)
        #ab <- gsub ("[[:punct:]]", "", ab)
        ab <- str_replace_all (ab, stops, repl)
        ab <- str_replace_all (ab, "[[:punct:]]", "")
        abstracts [y] <- ab
    }
    names (abstracts) <- years
    return (abstracts)
}
abstracts <- collate_abstracts (a)
```

Then convert to word-frequency tables and correlate inter-annual frequencies
between word frequencies (for all words with freq > 1).
```{r annual-correlations, messages = FALSE}
library (ggplot2)
abstract_freqs <- lapply (abstracts, function (i) {
            res <- table (paste0 (tolower (tokens (i))))
            freqs <- as.integer (res)
            names (freqs) <- names (res)
            res <- stats::aggregate (freqs, by = list (names (freqs)), FUN = sum)
            names (res) <- c ("token", "n")
            return (res)
            })
a1 <- abstract_freqs [-length (abstract_freqs)]
a2 <- abstract_freqs [-1]
r2 <- rep (NA, length (a1))
for (i in seq_along (a1)) {
    a1i <- a1 [[i]] [a1 [[i]]$n > 1, ]
    a2i <- a2 [[i]] [a2 [[i]]$n > 1, ]
    a <- dplyr::inner_join (a1i, a2i, by = "token")
    r2 [i] <- cor (a [, 2], a [, 3])
}
dat <- data.frame (year = as.integer (names (a2)),
                   r2 = r2 ^ 2)
ggplot (dat, aes (x = year, y = r2)) +
    geom_point (col = "#D24373", cex = 2) +
    geom_line (col = "#FF4373") +
    ylab ("Similarity in abstract texts to previous year") +
    theme (axis.title.y = element_text (angle = 90))
```

And that finally reveals one useful insights: The abstracts have over time
become very notably *more* similar, and converted towards a very pronounced
uniformity (R<sup>2</sup> > `r signif (tail (dat$r2, 1), 2)`).

## Unique Words

The uniformity, and the generic nature of the wordclouds and topic models, may
indicate that differences between abstracts actually arise in their focal nouns
or verbs, and that these are actually used very infrequently. The following
analysis extracts the unique words from each abstract.

```{r unique-words}
library (quanteda)
library (spacyr)
spacy_initialize ()
a <- readRDS ("jss-abstracts.Rds")
toks <- paste0 (a$abstract, collapse = " ") %>%
    tolower () %>%
    tokens () %>%
    paste0 ()
toks <- toks [!toks %in% stopwords () & nchar (toks) > 2]
# remove a few non-words
toks <- toks [!grepl ("^www|[1-9]", toks)]
temp <- table (toks)
tok_n <- as.integer (temp)
names (tok_n) <- names (temp)
toks1 <- spacy_parse (names (tok_n [tok_n == 1]))
verbs <- toks1$token [grep ("VERB", toks1$pos)]
nouns <- toks1$token [grep ("NOUN", toks1$pos)]
```

Then take the nouns and construct a network by measuring textual positions
between each. Note that `spacy_parse` parses some uniquely identified tokens
into multiple spacy-equivalent tokens, and so not all of these are necessarily
unique. This code generates a distance matrix of the minimal textual distance
between each pair of unique words within the entire corpus of abstracts. 


```{r word-network, eval = FALSE}
library (stringr)
nouns <- nouns [nchar (nouns) > 1]
res <- str_locate_all (paste0 (toks, collapse = " "), pattern = nouns)
for (i in seq_along (res)) res [[i]] <- cbind (res [[i]], i)
res <- data.frame (do.call (rbind, res) [, c (1, 3)])
res$noun <- nouns [res$i]
```
That then gives a `data.frame` with 3 columns of (1) start position of each of
the `r length (nouns)` nouns; an index `i` enumerating the nouns, and the nouns
themselves.
```{r textplot_network}
abstracts <- corpus (a$abstract) %>%
    tokens (remove_punct = TRUE) %>%
    tokens_tolower () %>%
    tokens_remove (pattern = stopwords ("English"))
# make dictionary of nouns
nouns <- unique (nouns)
dict <- as.list (nouns)
names (dict) <- nouns
dict <- dictionary (dict)
fcmat <- fcm (abstracts)
fcmat <- fcm_select (fcmat, pattern = dict)
rs <- rowSums (fcmat)
lens <- vapply (0:10, function (i) length (which (rs > i)), integer (1))
rsi <- (0:10) [which (lens < 1000) [1]]
index <- which (rs > rsi)
fcmat <- fcmat [index, index]
#textplot_network (fcmat, min_freq = 0.9)
```

The `textplot_network` function is unintelligible because it's static and
illegibly complex. Need to convert it to a dynamic representation, which the
following code does through constructing a `.js` representation suitable for
visualization via `visjs`. (Note that other options including the `networkD3`
and `sigmajs` packages are not at all responsive for networks of
this size.)


```{r make-visjs}
# convert the matrix to non-zero edges:
m <- as.matrix (fcmat)
m <- m + t (m) # render to symmetric matrix:
m [lower.tri (m)] <- 0 # reduce to upper tri only
diag (m) <- 0
res <- rep (list (NULL), nrow (m))
names (res) <- rownames (m)
for (i in seq (nrow (m))) {
    index <- which (m [i, ] > 0)
    res [[i]] <- m [i, index, drop = FALSE]
}
res <- res [which (lapply (res, length) > 0)]
from <- lapply (res, function (i) rep (rownames (i), ncol (i))) %>%
    unlist () %>%
    unname ()
to <- lapply (res, function (i) colnames (i)) %>%
    unlist () %>%
    unname ()
res <- data.frame (from = from,
                   to = to,
                   fromi = match (from, rownames (fcmat)),
                   toi = match (to, rownames (fcmat)),
                   n = unname (unlist (res)),
                   stringsAsFactors = FALSE)
# reduce to edges above defined weight limit:
wlim <- 100
res <- res [res$n > wlim, ]

# output the remaining nodes
nodes <- rbind (data.frame (node = res$from,
                            i = res$fromi,
                            stringsAsFactors = FALSE),
                data.frame (node = res$to,
                            i = res$toi,
                            stringsAsFactors = FALSE))
nodes <- nodes [which (!duplicated (nodes)), ]
# add results from clustering analysis below, to assign nodes to groups
nodes$group <- 0
#for (i in seq (cl)) {
#    nodes$group [which (nodes$node %in% cl [[i]])] <- i
#}
out <- "var nodes = new vis.DataSet(["
for (i in seq (nrow (nodes)))
    out <- c (out, paste0 ("    { id: ", nodes$i [i], ",",
                           "group: ", nodes$group [i], ",",
                           "label: \"", nodes$node [i], "\" },"))

out <- c (out, "]);", "", "", "var edges = new vis.DataSet([")
res_str <- apply (res, 1, function (i)
                  paste0 ("    { from: ", i [3],
                           ", to: ", i [4],
                           #", arrows: \"to\"",
                           ", value: ", i [5], " },"))
out <- c (out, res_str, "]);")

out <- c (out, "", "",
          "var container = document.getElementById(\"mynetwork\");",
          "var data = {",
          "    nodes: nodes,",
          "    edges: edges",
          "};",
          "var options = {",
          "    nodes: {",
          "        shape: \"box\",",
          "        margin: 10,",
          "        widthConstraint: {",
          "            maximum: 200",
          "        },",
          "        shadow: true",
          "    },",
          "    edges: {",
          "        shadow: true,",
          "        smooth: true",
          "    },",
          "    physics: {",
          "        barnesHut: { gravitationalConstant: -20000 },",
          "        stabilization: { iterations: 2500 }",
          "    }",
          "};",
          "",
          "",
          "var network = new vis.Network(container, data, options);")
```
dump that to a file
```{r, eval = FALSE}
con <- file ("./network.js", "w")
writeLines (out, con = con)
close (con)
```
That works, at least with the high threshold of `wlim = 100`, but the initial
rendering is still quite slow, and so there seems no way to visualise anything
like the full network. That threshold results in 81 nodes and 253 edges.
Dropping to `wlim = 50` increases those to 140 nodes and 616 edges. The
resultant network does appear after some time, but is the quite hard to
interpret intelligibly at all.

## Network clustering

A "cheaper" way to view relationships within the network might be to generate
simple clusters from the feature co-occurrence matrix. This can be done by
adapting the above code to generate a relationship matrix between the nodes,
and use that to define clusters. The following code applies `dbscan`
iteratively until all clusters have < 100 members, and automatically adjusted
the `eps` parameter to achieve the greatest numbers of clusters that are most
evenly distributed.

```{r net-clustering}
library (dbscan)
# function to find optimal value of eps, as the value giving the longest
# table with smallest dominant cluster size This can't use `optimise`,
# because the response is not smooth
# @param m A dissimilarity matrix
opteps <- function (m, minPts = 10) {
    clstats <- function (m, eps, minPts) {
        vapply (eps, function (i) {
                       x <- dbscan (m, eps = i, minPts = minPts)
                       tab <- table (x$cluster)
                       if (length (tab) == 1)
                           ret <- -1
                       else
                           ret <- length (tab) / max (tab)
                       return (ret) }, numeric (1))
    }
    eps <- 10 ^ ((-10:-2) / 2)
    val <- clstats (m, eps, minPts = minPts)
    if (all (val < 0))
        return (NULL)

    eps <- eps [which.max (val)] + (-5:5) * eps [which.max (val)] / 5
    eps <- eps [eps > 0]
    val <- clstats (m, eps, minPts = minPts)
    eps [which.max (val)]
}

# Then the meta-function to divide fcmat into clusters between minPts and maxPts
cluster_net <- function (fcmat, minPts = 10, maxPts = 100) {
    m <- as.matrix (fcmat)
    m <- m + t (m) # render to symmetric matrix:
    #m [lower.tri (m)] <- 0 # reduce to upper tri only
    diag (m) <- 0
    m <- 1 - m / max (m) # convert to a measure of distance
    minPts <- 10
    eps <- opteps (m, minPts = minPts)
    x <- dbscan (m, eps = eps, minPts = minPts)
    s <- split (rownames (m), f = as.factor (x$cluster))

    index <- which (vapply (s, length, integer (1)) > 100)
    indivisible <- rep (FALSE, length (s))
    while (length (index) > 0) {
        sadd <- list ()
        indiv_add <- NULL
        for (i in index) {
            si <- s [[i]]
            index2 <- match (si, rownames (m))
            msub <- m [index2, index2]
            eps <- opteps (msub, minPts = minPts)
            if (is.null (eps)) {
                indiv_add <-  c (indiv_add, TRUE)
                sadd <- c (sadd, list (si))
            } else {
                x <- dbscan (msub, eps = eps, minPts = minPts)
                indiv_add <- c (indiv_add, rep (FALSE, length (unique (x$cluster))))
                sadd <- c (sadd, split (rownames (msub), as.factor (x$cluster)))
            }
        }
        s [index] <- NULL
        indivisible <- indivisible [!seq (indivisible) %in% index]
        s <- c (s, sadd)
        indivisible <- c (indivisible, indiv_add)
        index <- which (vapply (s, length, integer (1)) > 100)
        indiv_i <- seq (indivisible) [indivisible]
        index <- index [!index %in% indiv_i]
    }
    names (s) <- NULL

    return (s)
}

cl <- cluster_net (fcmat, minPts = 10, maxPts = 100)
print (cl)
```

Those results can then be fed back in to the above `visjs` graph, although
doing that only reveals that almost all of the highly-connected groups belong
to one single cluster, so it doesn't help much.

